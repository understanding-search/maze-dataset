name: All Tokenizer Checks

on:
  push:
    paths:
      - 'maze_dataset/utils.py' # temporary
      - 'maze_dataset/token_utils.py' # temporary
      - 'maze_dataset/constants.py'
      - 'maze_dataset/tokenization/*.py'
      - 'maze_dataset/tokenization/MazeTokenizerModular_hashes.npz'
      - 'notebooks/demo_mazetokenizermodular.ipynb'
      - 'tests/all_tokenizers/*.py'
      - 'pyproject.toml' # on new version or update deps
      - '.github/workflows/all-tok-checks.yml' # changing this file
      - '.lastversion' # on new release
  workflow_dispatch:
    inputs:
      n_to_test:
        description: 'Number of tokenizers to test'
        required: false
        default: 300
        type: number
      pytest_parallel:
        description: '1 to parallelize tests with -n auto, to run without parallelization'
        required: false
        default: 1
        type: number

jobs:
  all_tok_test:
    name: All Tokenizer Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
        
      - name: Install dependencies
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          poetry lock --check
          poetry install
      - name: tokenizer hash tests
        run: |
          make test_tok_hashes
      - name: all tokenizer tests
        run: |
          N_TO_TEST=${{ github.event.inputs.n_to_test || '300' }}
          PYTEST_PARALLEL=${{ github.event.inputs.pytest_parallel || '1' }}
          make test_all_tok SKIP_HASH_TEST=1 NUM_TOKENIZERS_TO_TEST=$N_TO_TEST PYTEST_PARALLEL=$PYTEST_PARALLEL