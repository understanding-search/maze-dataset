{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from maze_dataset.tokenization import (\n",
    "    MazeTokenizerModular,\n",
    "    TokenizerElement,\n",
    "    TokenizationMode,\n",
    "    CoordTokenizers,\n",
    "    PromptSequencers,\n",
    ")\n",
    "\n",
    "from maze_dataset import (\n",
    "    VOCAB,\n",
    "    VOCAB_LIST,\n",
    "    VOCAB_TOKEN_TO_INDEX,\n",
    "    LatticeMazeGenerators,\n",
    "    MazeDataset,\n",
    "    MazeDatasetConfig,\n",
    ")\n",
    "\n",
    "from maze_dataset.plotting import MazePlot\n",
    "\n",
    "from maze_dataset.util import equal_except_adj_list_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MazeTokenizerModular` Initialization\n",
    "\n",
    "Most of the API for these tokenizers is contained in the `MazeTokenizerModular` class. The only time when users need to interact with the internal components of a `MazeTokenizerModular` is when initializing a non-default tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_default = MazeTokenizerModular()\n",
    "mt_ctt = MazeTokenizerModular.from_legacy(TokenizationMode.AOTP_CTT_indexed)\n",
    "# mt_default, mt_ctt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal components of `MazeTokenizerModular` are all instances of `TokenizerElement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers._CoordTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings._EdgeGrouping'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters._EdgePermuter'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets._EdgeSubset'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers._AdjListTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers._TargetTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes._StepSize'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers._PathTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers._PromptSequencer'>\n",
      "\n",
      "No-arg tokenizer defaults:\n",
      "AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\n",
      "UT()\n",
      "AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords())\n",
      "Ungrouped(connection_token_ordinal=1)\n",
      "ConnectionEdges(walls=F)\n",
      "RandomCoords()\n",
      "Unlabeled(post=F)\n",
      "StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F)\n",
      "Singles()\n",
      "Coord()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(elem) for elem in TokenizerElement.__subclasses__()]))\n",
    "print('\\nNo-arg tokenizer defaults:')\n",
    "print(\"\\n\".join([str(elem) for elem in mt_default._tokenizer_elements]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some `TokenizerElement`s also contain their own initialization arguments. Currently, all of these arguments are `boolean`-typed, but more complex args could be added in the future. The most common arguments across all `TokenizerElement`s are `pre`, `intra`, and `post`, which all control the option to add delimiter tokens to that part of the output. Other args are more specialized; see the docstrings for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "All instances of `MazeTokenizerModular` uses a static vocabulary `VOCAB`, which is one of the main functional differences from `MazeTokenizer`. Direct access to the static vocabulary can be made through 3 constants:\n",
    "- `VOCAB`: extension of the existing `SPECIAL_TOKENS` dataclass\n",
    "- `VOCAB_LIST: list[str]`\n",
    "- `VOCAB_TOKEN_TO_INDEX: dict[str, int]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`VOCAB`: type\n",
      "\tADJLIST_START\n",
      "\tADJLIST_END\n",
      "\tTARGET_START\n",
      "\tTARGET_END\n",
      "\tORIGIN_START\n",
      "\n",
      "`VOCAB_LIST`: list[str]\n",
      "\t<ADJLIST_START>\n",
      "\t<ADJLIST_END>\n",
      "\t<TARGET_START>\n",
      "\t<TARGET_END>\n",
      "\t<ORIGIN_START>\n",
      "\n",
      "`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\n",
      "\t<ADJLIST_START>:   \t0\n",
      "\t<ADJLIST_END>:   \t1\n",
      "\t<TARGET_START>:   \t2\n",
      "\t<TARGET_END>:   \t3\n",
      "\t<ORIGIN_START>:   \t4\n"
     ]
    }
   ],
   "source": [
    "print(\"`VOCAB`: type\")\n",
    "for i, t in enumerate(VOCAB):\n",
    "    if i >= 5: break\n",
    "    print(f\"\\t{t}\")\n",
    "\n",
    "print(\"\\n`VOCAB_LIST`: list[str]\")\n",
    "for t in VOCAB_LIST[:5]:\n",
    "    print(f\"\\t{t}\")\n",
    "    \n",
    "print(\"\\n`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\")\n",
    "for t in VOCAB_TOKEN_TO_INDEX:\n",
    "    if VOCAB_TOKEN_TO_INDEX[t] >= 5: break\n",
    "    print(f\"\\t{t}:   \\t{VOCAB_TOKEN_TO_INDEX[t]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations of Static Vocabulary\n",
    "\n",
    "- No more rasterized vs uniform indexing\n",
    "- Fixed max grid size\n",
    "  - There is now a fixed maximum maze size which is supported.\n",
    "  - Unique tokens (`CoordTokenizers.UT`): 50x50\n",
    "  - Coordinate tuple tokens (`CoordTokenizers.CTT`): 128x128\n",
    "  - Mazes larger than these sizes are not supported\n",
    "  - There should be fewer compatibility issues with tokenizers using different `max_grid_size` parameters\n",
    "- Vocabulary access\n",
    "  - There is now no need to pass around a tokenizer object or any parameter to access its custom vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring your code from legacy `MazeTokenizer` and `TokenizationMode`\n",
    "Since `MazeTokenizerModular` uses a static vocabulary, it is not backwards compatible with any models trained using a legacy `MazeTokenizer`. The Unsearch libraries will soon be updated to use `MazeTokenizerModular` by default in all settings. \n",
    "\n",
    "If you've manually specified a `MazeTokenizer` or `TokenizationMode` in your research code, the easiest way to refactor is using `MazeTokenizerModular.from_legacy`, which will convert a `MazeTokenizer` or `TokenizationMode` to its corresponding `MazeTokenizerModular` instance. Note that this correspondence means only that the tokenization of mazes to strings are equivalent; the encodings to integer vocabulary indices are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ALL_TOKENIZERS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, all combinations of `TokenizerElement`s and their arguments will produce a valid and unique `MazeTokenizerModular`. However, it is not guaranteed that every possible `MazeTokenizerModular` that can be constructed will make practical sense or have been put through testing. `ALL_TOKENIZERS` contains all the tested tokenizers pre-built. For research investigating many different tokenization schemes, one practical way to access them is by looping through `ALL_TOKENIZERS`. Be aware that the indexing of specific tokenizers in `ALL_TOKENIZERS` may change in any minor update without notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>,)\n",
      "kwargs:\t{'validation_funcs': frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>})}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers._PromptSequencer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers._CoordTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers.UT'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers.CTT'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers._AdjListTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers.AdjListCoord'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings._EdgeGrouping'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings.Ungrouped'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(typing.Literal[0, 1, 2], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings.ByLeadingCoord'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(typing.Literal[0, 1], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets._EdgeSubset'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets.AllLatticeEdges'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets.ConnectionEdges'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters._EdgePermuter'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters.SortedCoords'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters.RandomCoords'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters.BothCoords'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers.AdjListCardinal'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings._EdgeGrouping'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets._EdgeSubset'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters._EdgePermuter'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers._TargetTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers.Unlabeled'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers._PathTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers.StepSequence'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes._StepSize'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes.Singles'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes.Straightaways'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes.Forks'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes.ForksAndStraightaways'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Coord'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Cardinal'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Relative'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Distance'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer], frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n",
      "args:\t(<class 'bool'>, frozendict.frozendict({<class 'maze_dataset.tokenization.maze_tokenizer.TokenizerElement'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793D990>, <class 'maze_dataset.tokenization.maze_tokenizer.MazeTokenizerModular'>: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DA20>, tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer] | tuple[maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer, maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer]: <function _get_all_tokenizers.<locals>.<lambda> at 0x000002480793DAB0>}))\n",
      "kwargs:\t{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\tokenization\\maze_tokenizer.py:934: UserWarning: no validation implemented for AdjListTokenizers\n",
      "  warnings.warn(\n",
      "c:\\Users\\aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-h6YQ-YyV-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare Coord() and Distance() for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Coord'> vs <class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Distance'>\n",
      "  warnings.warn(\n",
      "c:\\Users\\aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-h6YQ-YyV-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare Distance() and Coord() for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Distance'> vs <class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Coord'>\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m interrupt_thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39msend_interrupt)\n\u001b[0;32m     14\u001b[0m interrupt_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m---> 16\u001b[0m ALL_TOKENIZERS \u001b[38;5;241m=\u001b[39m \u001b[43m_get_all_tokenizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\tokenization\\all_tokenizers.py:49\u001b[0m, in \u001b[0;36m_get_all_tokenizers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;129m@cache\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_all_tokenizers\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[MazeTokenizerModular]:\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    Computes a complete list of all valid tokenizers.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Warning: This is an expensive function.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mall_instances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMazeTokenizerModular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrozendict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozendict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTokenizerElement\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                \u001b[49m\u001b[43mMazeTokenizerModular\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[43mStepTokenizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStepTokenizerPermutation\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mStepTokenizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:518\u001b[0m, in \u001b[0;36m_all_instances_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     validation_funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_validation_func(args[\u001b[38;5;241m0\u001b[39m], \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[43m)\u001b[49m, validation_funcs)\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:567\u001b[0m, in \u001b[0;36mall_instances\u001b[1;34m(type_, validation_funcs)\u001b[0m\n\u001b[0;32m    564\u001b[0m     fields: \u001b[38;5;28mlist\u001b[39m[field] \u001b[38;5;241m=\u001b[39m type_\u001b[38;5;241m.\u001b[39m__dataclass_fields__\n\u001b[0;32m    565\u001b[0m     fields_to_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {f: fields[f]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields}\n\u001b[0;32m    566\u001b[0m     all_arg_sequences: Iterable \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[1;32m--> 567\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    568\u001b[0m             all_instances(arg_type, validation_funcs)\n\u001b[0;32m    569\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m arg_type \u001b[38;5;129;01min\u001b[39;00m fields_to_types\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    570\u001b[0m         ]\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:568\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    564\u001b[0m     fields: \u001b[38;5;28mlist\u001b[39m[field] \u001b[38;5;241m=\u001b[39m type_\u001b[38;5;241m.\u001b[39m__dataclass_fields__\n\u001b[0;32m    565\u001b[0m     fields_to_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {f: fields[f]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields}\n\u001b[0;32m    566\u001b[0m     all_arg_sequences: Iterable \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m--> 568\u001b[0m             \u001b[43mall_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m arg_type \u001b[38;5;129;01min\u001b[39;00m fields_to_types\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    570\u001b[0m         ]\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:518\u001b[0m, in \u001b[0;36m_all_instances_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     validation_funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_validation_func(args[\u001b[38;5;241m0\u001b[39m], \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[43m)\u001b[49m, validation_funcs)\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:580\u001b[0m, in \u001b[0;36mall_instances\u001b[1;34m(type_, validation_funcs)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    579\u001b[0m         flatten(\n\u001b[1;32m--> 580\u001b[0m             [\n\u001b[0;32m    581\u001b[0m                 all_instances(sub, validation_funcs)\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m type_\u001b[38;5;241m.\u001b[39m__subclasses__()\n\u001b[0;32m    583\u001b[0m             ],\n\u001b[0;32m    584\u001b[0m             levels_to_flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    585\u001b[0m         )\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    588\u001b[0m     get_origin(type_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    589\u001b[0m ):  \u001b[38;5;66;03m# Only matches Generic type tuple since regular tuple is not finite-valued\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# Generic tuple: Similar to concrete dataclass. Construct all possible combinations of tuple fields.\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(combo)\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m         )\n\u001b[0;32m    599\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:581\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    579\u001b[0m         flatten(\n\u001b[0;32m    580\u001b[0m             [\n\u001b[1;32m--> 581\u001b[0m                 \u001b[43mall_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m type_\u001b[38;5;241m.\u001b[39m__subclasses__()\n\u001b[0;32m    583\u001b[0m             ],\n\u001b[0;32m    584\u001b[0m             levels_to_flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    585\u001b[0m         )\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    588\u001b[0m     get_origin(type_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    589\u001b[0m ):  \u001b[38;5;66;03m# Only matches Generic type tuple since regular tuple is not finite-valued\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# Generic tuple: Similar to concrete dataclass. Construct all possible combinations of tuple fields.\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(combo)\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m         )\n\u001b[0;32m    599\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:518\u001b[0m, in \u001b[0;36m_all_instances_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     validation_funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_validation_func(args[\u001b[38;5;241m0\u001b[39m], \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_funcs\u001b[49m\u001b[43m)\u001b[49m, validation_funcs)\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:572\u001b[0m, in \u001b[0;36mall_instances\u001b[1;34m(type_, validation_funcs)\u001b[0m\n\u001b[0;32m    565\u001b[0m     fields_to_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {f: fields[f]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields}\n\u001b[0;32m    566\u001b[0m     all_arg_sequences: Iterable \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    568\u001b[0m             all_instances(arg_type, validation_funcs)\n\u001b[0;32m    569\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m arg_type \u001b[38;5;129;01min\u001b[39;00m fields_to_types\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    570\u001b[0m         ]\n\u001b[0;32m    571\u001b[0m     )\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    579\u001b[0m         flatten(\n\u001b[0;32m    580\u001b[0m             [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         )\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\NTFS\\dev\\Unsearch\\maze-dataset\\maze_dataset\\utils.py:573\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    565\u001b[0m     fields_to_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m {f: fields[f]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields}\n\u001b[0;32m    566\u001b[0m     all_arg_sequences: Iterable \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    568\u001b[0m             all_instances(arg_type, validation_funcs)\n\u001b[0;32m    569\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m arg_type \u001b[38;5;129;01min\u001b[39;00m fields_to_types\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    570\u001b[0m         ]\n\u001b[0;32m    571\u001b[0m     )\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 573\u001b[0m         type_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{fld: arg \u001b[38;5;28;01mfor\u001b[39;00m fld, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fields_to_types\u001b[38;5;241m.\u001b[39mkeys(), args)})\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m all_arg_sequences\n\u001b[0;32m    575\u001b[0m     ]\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataclass_fields__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_abstract(type_):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# Abstract dataclass: call `all_instances` on each subclass\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    579\u001b[0m         flatten(\n\u001b[0;32m    580\u001b[0m             [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         )\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32m<string>:4\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, coord_tokenizer, adj_list_tokenizer, target_tokenizer, path_tokenizer)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: this cell fails because all_tokenizers is broken and takes too long. we kill it after a minute to give helpful error messages of where the long (infinite?) loop is.\n",
    "from maze_dataset.tokenization.all_tokenizers import _get_all_tokenizers\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import signal\n",
    "\n",
    "# interrupt after a fixed time\n",
    "def send_interrupt():\n",
    "\ttime.sleep(5)\n",
    "\tsignal.raise_signal(signal.SIGINT)\n",
    "\n",
    "interrupt_thread = threading.Thread(target=send_interrupt)\n",
    "interrupt_thread.start()\n",
    "\n",
    "ALL_TOKENIZERS = _get_all_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get the dataset 'test-g3-n1-a_dfs-h84654'\n",
      "generating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating & solving mazes: 100%|| 1/1 [00:00<00:00, 111.14maze/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset test with 1 items. output.cfg.to_fname() = 'test-g3-n1-a_dfs-h84654'\n",
      "\n",
      "\n",
      "MazeTokenizerModular-PromptSequencers.AOTP()-CoordTokenizers.UT()-AdjListTokenizers.Coords(intra=True, post=True, walls=False)-TargetTokenizers.Unlabeled(post=False)-PathTokenizers.Coords(post=False)\n",
      "['<ADJLIST_START>', '(0,0)', '<-->', '(1,0)', ';', '(2,0)', '<-->', '(2,1)', ';', '(2,2)', '<-->', '(2,1)', ';', '(1,1)', '<-->', '(0,1)', ';', '(1,2)', '<-->', '(2,2)', ';', '(1,1)', '<-->', '(1,2)', ';', '(1,2)', '<-->', '(0,2)', ';', '(2,0)', '<-->', '(1,0)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(1,2)', '<ORIGIN_END>', '<TARGET_START>', '(0,0)', '<TARGET_END>', '<PATH_START>', '(1,2)', '(2,2)', '(2,1)', '(2,0)', '(1,0)', '(0,0)', '<PATH_END>']\n",
      "['<ADJLIST_START>', '(0,0)', '<-->', '(1,0)', ';', '(1,1)', '<-->', '(0,1)', ';', '(1,2)', '<-->', '(2,2)', ';', '(1,1)', '<-->', '(1,2)', ';', '(1,2)', '<-->', '(0,2)', ';', '(2,1)', '<-->', '(2,0)', ';', '(2,0)', '<-->', '(1,0)', ';', '(2,1)', '<-->', '(2,2)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(1,2)', '<ORIGIN_END>', '<TARGET_START>', '(0,0)', '<TARGET_END>', '<PATH_START>', '(1,2)', '(2,2)', '(2,1)', '(2,0)', '(1,0)', '(0,0)', '<PATH_END>']\n",
      "\n",
      "MazeTokenizerModular-PromptSequencers.AOTP()-CoordTokenizers.CTT(pre=True, intra=True, post=True)-AdjListTokenizers.Coords(intra=True, post=True, walls=False)-TargetTokenizers.Unlabeled(post=False)-PathTokenizers.Coords(post=False)\n",
      "['<ADJLIST_START>', '(', '2', ',', '2', ')', '<-->', '(', '2', ',', '1', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '1', ',', '1', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '1', ',', '0', ')', '<-->', '(', '2', ',', '0', ')', ';', '(', '2', ',', '1', ')', '<-->', '(', '2', ',', '0', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '2', ',', '2', ')', ';', '(', '0', ',', '1', ')', '<-->', '(', '1', ',', '1', ')', ';', '(', '1', ',', '0', ')', '<-->', '(', '0', ',', '0', ')', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(', '1', ',', '2', ')', '<ORIGIN_END>', '<TARGET_START>', '(', '0', ',', '0', ')', '<TARGET_END>', '<PATH_START>', '(', '1', ',', '2', ')', '(', '2', ',', '2', ')', '(', '2', ',', '1', ')', '(', '2', ',', '0', ')', '(', '1', ',', '0', ')', '(', '0', ',', '0', ')', '<PATH_END>']\n",
      "['<ADJLIST_START>', '(', '0', ',', '0', ')', '<-->', '(', '1', ',', '0', ')', ';', '(', '2', ',', '0', ')', '<-->', '(', '2', ',', '1', ')', ';', '(', '2', ',', '2', ')', '<-->', '(', '2', ',', '1', ')', ';', '(', '0', ',', '2', ')', '<-->', '(', '1', ',', '2', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '2', ',', '2', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '1', ',', '1', ')', ';', '(', '2', ',', '0', ')', '<-->', '(', '1', ',', '0', ')', ';', '(', '0', ',', '1', ')', '<-->', '(', '1', ',', '1', ')', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(', '1', ',', '2', ')', '<ORIGIN_END>', '<TARGET_START>', '(', '0', ',', '0', ')', '<TARGET_END>', '<PATH_START>', '(', '1', ',', '2', ')', '(', '2', ',', '2', ')', '(', '2', ',', '1', ')', '(', '2', ',', '0', ')', '(', '1', ',', '0', ')', '(', '0', ',', '0', ')', '<PATH_END>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cfg: MazeDatasetConfig = MazeDatasetConfig(\n",
    "    name=\"test\",\n",
    "    grid_n=3,\n",
    "    n_mazes=1,\n",
    "    maze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    ")\n",
    "maze_dataset: MazeDataset = MazeDataset.from_config(\n",
    "    cfg,\n",
    "    do_download=False,\n",
    "    load_local=False,\n",
    "    do_generate=True,\n",
    "    save_local=False,\n",
    "    verbose=True,\n",
    "    gen_parallel=False,\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "for tokenizer in ALL_TOKENIZERS:\n",
    "    print(tokenizer)\n",
    "    a: list[str] = maze_dataset[0].as_tokens(tokenizer)\n",
    "    b: list[str] = tokenizer.to_tokens(maze_dataset[0])\n",
    "    assert equal_except_adj_list_sequence(a, b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible tokenizers which aren't in `ALL_TOKENIZERS` are not guaranteed to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-zQQMZP3O-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare PromptSequencers.AOTP() and PromptSequencers.AOP(include_target_special_tokens=False) for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'> vs <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOP'>\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-zQQMZP3O-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare PromptSequencers.AOP(include_target_special_tokens=False) and PromptSequencers.AOTP() for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOP'> vs <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = MazeTokenizerModular(\n",
    "    prompt_sequencer=PromptSequencers.AOP(include_target_special_tokens=False),\n",
    ")\n",
    "\n",
    "assert custom_tokenizer not in ALL_TOKENIZERS  # Danger, use at your own risk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL_TOKENIZERS Reference\n",
    "\n",
    "For each tokenizer, tokenizations and encodings of the below maze are logged in a DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MazePlot(maze_dataset[0]).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoding</th>\n",
       "      <th>prompt_sequencer</th>\n",
       "      <th>coord_tokenizer</th>\n",
       "      <th>adj_list_tokenizer</th>\n",
       "      <th>target_tokenizer</th>\n",
       "      <th>path_tokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MazeTokenizerModular-PromptSequencers.AOTP()-CoordTo...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; (1,2) &lt;--&gt; (0,2) ; (0,0) &lt;--&gt; ...</td>\n",
       "      <td>[0, 1602, 8, 1600, 9, 1596, 8, 1598, 9, 1602, ...</td>\n",
       "      <td>PromptSequencers.AOTP()</td>\n",
       "      <td>CoordTokenizers.UT()</td>\n",
       "      <td>AdjListTokenizers.Coords(intra=True, post=True...</td>\n",
       "      <td>TargetTokenizers.Unlabeled(post=False)</td>\n",
       "      <td>PathTokenizers.Coords(post=False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MazeTokenizerModular-PromptSequencers.AOTP()-CoordTo...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; ( 1 , 1 ) &lt;--&gt; ( 0 , 1 ) ; ( 2...</td>\n",
       "      <td>[0, 11, 321, 12, 321, 13, 8, 11, 320, 12, 321,...</td>\n",
       "      <td>PromptSequencers.AOTP()</td>\n",
       "      <td>CoordTokenizers.CTT(pre=True, intra=True, post...</td>\n",
       "      <td>AdjListTokenizers.Coords(intra=True, post=True...</td>\n",
       "      <td>TargetTokenizers.Unlabeled(post=False)</td>\n",
       "      <td>PathTokenizers.Coords(post=False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenizer  \\\n",
       "0  MazeTokenizerModular-PromptSequencers.AOTP()-CoordTo...   \n",
       "1  MazeTokenizerModular-PromptSequencers.AOTP()-CoordTo...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  <ADJLIST_START> (1,2) <--> (0,2) ; (0,0) <--> ...   \n",
       "1  <ADJLIST_START> ( 1 , 1 ) <--> ( 0 , 1 ) ; ( 2...   \n",
       "\n",
       "                                            encoding         prompt_sequencer  \\\n",
       "0  [0, 1602, 8, 1600, 9, 1596, 8, 1598, 9, 1602, ...  PromptSequencers.AOTP()   \n",
       "1  [0, 11, 321, 12, 321, 13, 8, 11, 320, 12, 321,...  PromptSequencers.AOTP()   \n",
       "\n",
       "                                     coord_tokenizer  \\\n",
       "0                               CoordTokenizers.UT()   \n",
       "1  CoordTokenizers.CTT(pre=True, intra=True, post...   \n",
       "\n",
       "                                  adj_list_tokenizer  \\\n",
       "0  AdjListTokenizers.Coords(intra=True, post=True...   \n",
       "1  AdjListTokenizers.Coords(intra=True, post=True...   \n",
       "\n",
       "                         target_tokenizer                     path_tokenizer  \n",
       "0  TargetTokenizers.Unlabeled(post=False)  PathTokenizers.Coords(post=False)  \n",
       "1  TargetTokenizers.Unlabeled(post=False)  PathTokenizers.Coords(post=False)  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers: pd.DataFrame = pd.DataFrame( \n",
    "    columns=[\"tokenizer\", \"tokens\", \"encoding\", *mt_default.summary().keys()]\n",
    "    )\n",
    "tokenizers.tokenizer = ALL_TOKENIZERS\n",
    "tokenizers.tokens = tokenizers.tokenizer.apply(lambda x: \" \".join(maze_dataset[0].as_tokens(x)))\n",
    "tokenizers.encoding = tokenizers.tokens.apply(mt_default.encode)\n",
    "for k in mt_default.summary().keys():\n",
    "    tokenizers[k] = tokenizers.tokenizer.apply(lambda x: getattr(x, k))\n",
    "\n",
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-dataset-zQQMZP3O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
