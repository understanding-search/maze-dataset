{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "from tqdm import tqdm\n",
    "\n",
    "from maze_dataset import (\n",
    "\tVOCAB,\n",
    "\tVOCAB_LIST,\n",
    "\tVOCAB_TOKEN_TO_INDEX,\n",
    "\tLatticeMazeGenerators,\n",
    "\tMazeDataset,\n",
    "\tMazeDatasetConfig,\n",
    "\tSolvedMaze,\n",
    ")\n",
    "from maze_dataset.plotting import MazePlot\n",
    "from maze_dataset.tokenization import (\n",
    "\tAdjListTokenizers,\n",
    "\tCoordTokenizers,\n",
    "\tEdgePermuters,\n",
    "\tEdgeSubsets,\n",
    "\tMazeTokenizer,\n",
    "\tMazeTokenizerModular,\n",
    "\tPathTokenizers,\n",
    "\tPromptSequencers,\n",
    "\tStepSizes,\n",
    "\tStepTokenizers,\n",
    "\tTargetTokenizers,\n",
    "\tTokenizationMode,\n",
    "\t_TokenizerElement,\n",
    ")\n",
    "from maze_dataset.tokenization.modular.all_instances import all_instances\n",
    "from maze_dataset.tokenization.modular.all_tokenizers import (\n",
    "\tMAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,\n",
    "\tget_all_tokenizers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MazeTokenizerModular` Initialization and Structure\n",
    "\n",
    "Initialiation can be done vai the default constructor or via `MazeTokenizerModular.from_legacy`. The latter is useful for converting a legacy `MazeTokenizer` into its equivalent `MazeTokenizerModular`.\n",
    "\n",
    "Most of the API for these tokenizers is contained in the `MazeTokenizerModular` class. The only time when users need to interact with the internal components of a `MazeTokenizerModular` is when initializing a non-default tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_default: MazeTokenizerModular = MazeTokenizerModular()\n",
    "mt_ctt: MazeTokenizerModular = MazeTokenizerModular.from_legacy(\n",
    "\tTokenizationMode.AOTP_CTT_indexed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects composing `MazeTokenizerModular` are all instances of `_TokenizerElement`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'maze_dataset.tokenization.modular.elements.CoordTokenizers._CoordTokenizer'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.EdgeGroupings._EdgeGrouping'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.EdgePermuters._EdgePermuter'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.EdgeSubsets._EdgeSubset'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.AdjListTokenizers._AdjListTokenizer'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.TargetTokenizers._TargetTokenizer'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.StepSizes._StepSize'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.StepTokenizers._StepTokenizer'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.PathTokenizers._PathTokenizer'>\n",
      "<class 'maze_dataset.tokenization.modular.elements.PromptSequencers._PromptSequencer'>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(elem) for elem in _TokenizerElement.__subclasses__()]))\n",
    "assert all(\n",
    "\tissubclass(elem, _TokenizerElement) for elem in _TokenizerElement.__subclasses__()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a tokenizer, these `_TokenizerElement`s are structured in a nested dataclass tree. The tree is slightly different depending on the particular options selected. Below are shown 3 different tree representations of `mt_default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AOTP `_TokenizerElement` Structure:\n",
      "\n",
      "MazeTokenizerModular\n",
      "\t_PromptSequencer\n",
      "\t\t_CoordTokenizer\n",
      "\t\t_AdjListTokenizer\n",
      "\t\t\t_EdgeGrouping\n",
      "\t\t\t_EdgeSubset\n",
      "\t\t\t_EdgePermuter\n",
      "\t\t_TargetTokenizer\n",
      "\t\t_PathTokenizer\n",
      "\t\t\t_StepSize\n",
      "\t\t\t_StepTokenizer\n",
      "\n",
      "Default tokenizer elements:\n",
      "\n",
      "MazeTokenizerModular\n",
      "\tAOTP\n",
      "\t\tUT\n",
      "\t\tAdjListCoord\n",
      "\t\t\tUngrouped\n",
      "\t\t\tConnectionEdges\n",
      "\t\t\tRandomCoords\n",
      "\t\tUnlabeled\n",
      "\t\tStepSequence\n",
      "\t\t\tSingles\n",
      "\t\t\tCoord\n",
      "\n",
      "\n",
      "Default tokenizer `name`:\n",
      "\n",
      "MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\n",
      "`MazeTokenizerModular` structure with all fields:\n",
      "\n",
      "MazeTokenizerModular:\n",
      "  AOTP:\n",
      "    adj_list_tokenizer:\n",
      "      AdjListCoord:\n",
      "        edge_grouping:\n",
      "          Ungrouped:\n",
      "            connection_token_ordinal: 1\n",
      "        edge_permuter:\n",
      "          RandomCoords: {}\n",
      "        edge_subset:\n",
      "          ConnectionEdges:\n",
      "            walls: false\n",
      "        post: true\n",
      "        pre: false\n",
      "        shuffle_d0: true\n",
      "    coord_tokenizer:\n",
      "      UT: {}\n",
      "    path_tokenizer:\n",
      "      StepSequence:\n",
      "        intra: false\n",
      "        post: false\n",
      "        pre: false\n",
      "        step_size:\n",
      "          Singles: {}\n",
      "        step_tokenizers:\n",
      "        - Coord: {}\n",
      "    target_tokenizer:\n",
      "      Unlabeled:\n",
      "        post: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAOTP `_TokenizerElement` Structure:\\n\")\n",
    "print(mt_default.tokenizer_element_tree(abstract=True))\n",
    "print(\"Default tokenizer elements:\\n\")\n",
    "print(mt_default.tokenizer_element_tree())\n",
    "print(\"\\nDefault tokenizer `name`:\\n\")\n",
    "print(mt_default.name)\n",
    "print(\"`MazeTokenizerModular` structure with all fields:\\n\")\n",
    "print(yaml.dump(mt_default.tokenizer_element_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no other constructor methods. To construct a `MazeTokenizerModular` with other `TokenizerElement`s besides those available via `from_legacy`, the standard constructor with all parent `TokenizerElement`s in the tree must be used. Some `TokenizerElement`s also contain their own initialization arguments, most of which are `boolean`-typed. The most common arguments across all `TokenizerElement`s are named `pre`, `intra`, and `post`, which all control the option to add delimiter tokens to that part of the output. Other args are more specialized; see the class docstrings for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "All instances of `MazeTokenizerModular` uses a static vocabulary `VOCAB`, which is one of the main functional differences from `MazeTokenizer`. Direct access to the static vocabulary can be made through 3 constants:\n",
    "- `VOCAB`\n",
    "  - Extension of the `SPECIAL_TOKENS` dataclass\n",
    "  - Supports direct property attribution\n",
    "- `VOCAB_LIST: list[str]`\n",
    "  - Contains the vocabulary in a list\n",
    "  - Index of a token is its unique ID\n",
    "- `VOCAB_TOKEN_TO_INDEX: dict[str, int]`\n",
    "  - Inverse mapping of `VOCAB_LIST`, maps tokens to unique IDs\n",
    "\n",
    "The following shows a visualization of the first 5 elements of each constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`VOCAB`: IsDataclass\n",
      "\tVOCAB.ADJLIST_START =\t'<ADJLIST_START>'\n",
      "\tVOCAB.ADJLIST_END =\t'<ADJLIST_END>'\n",
      "\tVOCAB.TARGET_START =\t'<TARGET_START>'\n",
      "\tVOCAB.TARGET_END =\t'<TARGET_END>'\n",
      "\tVOCAB.ORIGIN_START =\t'<ORIGIN_START>'\n",
      "\t...\n",
      "\n",
      "`VOCAB_LIST`: list[str]\n",
      "\t'<ADJLIST_START>'\n",
      "\t'<ADJLIST_END>'\n",
      "\t'<TARGET_START>'\n",
      "\t'<TARGET_END>'\n",
      "\t'<ORIGIN_START>'\n",
      "\t...\n",
      "\n",
      "`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\n",
      "\t'<ADJLIST_START>':   \t0\n",
      "\t'<ADJLIST_END>':   \t1\n",
      "\t'<TARGET_START>':   \t2\n",
      "\t'<TARGET_END>':   \t3\n",
      "\t'<ORIGIN_START>':   \t4\n",
      "\t...\n"
     ]
    }
   ],
   "source": [
    "print(\"`VOCAB`: IsDataclass\")\n",
    "for i, t in enumerate(VOCAB):\n",
    "\tif i >= 5:\n",
    "\t\tbreak\n",
    "\tprint(f\"\\tVOCAB.{t} =\\t'{getattr(VOCAB, t)}'\")\n",
    "print(\"\\t...\")\n",
    "\n",
    "print(\"\\n`VOCAB_LIST`: list[str]\")\n",
    "for t in VOCAB_LIST[:5]:\n",
    "\tprint(f\"\\t'{t}'\")\n",
    "print(\"\\t...\")\n",
    "\n",
    "print(\"\\n`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\")\n",
    "for t in VOCAB_TOKEN_TO_INDEX:\n",
    "\tif VOCAB_TOKEN_TO_INDEX[t] >= 5:\n",
    "\t\tbreak\n",
    "\tprint(f\"\\t'{t}':   \\t{VOCAB_TOKEN_TO_INDEX[t]}\")\n",
    "print(\"\\t...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations of Static Vocabulary\n",
    "\n",
    "- No more rasterized vs uniform indexing, it's all fixed as uniform now\n",
    "- Fixed max grid size\n",
    "  - There is now a fixed maximum maze size which is supported.\n",
    "  - Unique tokens (`CoordTokenizers.UT`): 50x50\n",
    "  - Coordinate tuple tokens (`CoordTokenizers.CTT`): 128x128\n",
    "  - Mazes larger than these sizes are not supported\n",
    "  - There should be fewer compatibility issues with tokenizers using different `max_grid_size` parameters\n",
    "- Vocabulary access\n",
    "  - Since maze-dataset 1.0, there is no need to pass around a tokenizer object or any data structure to access its custom vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring your code from legacy `MazeTokenizer` and `TokenizationMode`\n",
    "Since `MazeTokenizerModular` uses a static vocabulary, it is not backwards compatible with any models trained using a legacy `MazeTokenizer`. The `maze-transformer` library is updated in vX.X.X to use `MazeTokenizerModular` by default. \n",
    "\n",
    "If you've manually specified a `MazeTokenizer` or `TokenizationMode` in your research code, the easiest way to refactor is using `MazeTokenizerModular.from_legacy`, which will convert a `MazeTokenizer` or `TokenizationMode` to its corresponding `MazeTokenizerModular` instance. Note that this correspondence means only that the stringification of mazes are equivalent; the encodings of strings to integer vocabulary indices are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MazeTokenizer(tokenization_mode=<TokenizationMode.AOTP_UT_uniform: 'AOTP_UT_uniform'>, max_grid_size=None) \n",
      " MazeTokenizerModular(prompt_sequencer=PromptSequencers.AOTP(coord_tokenizer=CoordTokenizers.UT(), adj_list_tokenizer=AdjListTokenizers.AdjListCoord(pre=False, post=True, shuffle_d0=True, edge_grouping=EdgeGroupings.Ungrouped(connection_token_ordinal=1), edge_subset=EdgeSubsets.ConnectionEdges(walls=False), edge_permuter=EdgePermuters.RandomCoords()), target_tokenizer=TargetTokenizers.Unlabeled(post=False), path_tokenizer=PathTokenizers.StepSequence(step_size=StepSizes.Singles(), step_tokenizers=(StepTokenizers.Coord(),), pre=False, intra=False, post=False)))\n"
     ]
    }
   ],
   "source": [
    "legacy_maze_tokenizer: MazeTokenizer = (\n",
    "\tTokenizationMode.AOTP_UT_uniform.to_legacy_tokenizer()\n",
    ")\n",
    "modular_tokenizer_equivalent: MazeTokenizerModular = MazeTokenizerModular.from_legacy(\n",
    "\tlegacy_maze_tokenizer,\n",
    ")\n",
    "print(legacy_maze_tokenizer, \"\\n\", modular_tokenizer_equivalent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_all_tokenizers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most combinations of `TokenizerElement`s and their arguments will produce a valid and unique `MazeTokenizerModular`. However, it is not guaranteed that every possible `MazeTokenizerModular` that can be constructed will make practical sense or have been put through testing.\n",
    "\n",
    "`get_all_tokenizers` constructs and caches all the tested tokenizers at once. For research investigating many different tokenization schemes, one practical way to access them is by looping through/sampling from `get_all_tokenizers()`. Be aware that the indexing of specific tokenizers may change without notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenizers = get_all_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5878656 or 5.9M tokenizers found.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\tf\"{len(all_tokenizers)} or {shorten_numerical_to_str(len(all_tokenizers))} tokenizers found.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible tokenizers which aren't in `get_all_tokenizers` are not guaranteed to function. Instead of running the expensive call to `get_all_tokenizers` yourself, you can check if a tokenizer is tested using `MazeTokenizerModular.is_tested_tokenizer` or `MazeTokenizerModular.is_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mt_default.is_tested_tokenizer(do_except=True)\n",
    "assert mt_default.is_valid()\n",
    "assert mt_ctt.is_tested_tokenizer()\n",
    "assert mt_ctt.is_valid()\n",
    "\n",
    "custom_untested_tokenizer = MazeTokenizerModular(\n",
    "\tprompt_sequencer=PromptSequencers.AOP(\n",
    "\t\tpath_tokenizer=PathTokenizers.StepSequence(\n",
    "\t\t\tstep_tokenizers=(StepTokenizers.Distance(),),\n",
    "\t\t),\n",
    "\t),\n",
    ")\n",
    "\n",
    "assert not custom_untested_tokenizer.is_tested_tokenizer()\n",
    "assert not custom_untested_tokenizer.is_valid()\n",
    "# Danger, use this tokenizer at your own risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this uses below file, shipped with the package, to keep track of which tokenizer names are valid. the code for generating them is in `maze_dataset.tokenization.modular.fst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMT_FST_PATH = PosixPath('/home/miv/projects/mazes/maze-dataset/maze_dataset/tokenization/modular/MazeTokenizerModular_tested.fst')\n",
      "MMT_FST_PATH.stat().st_size = 1619\n"
     ]
    }
   ],
   "source": [
    "from maze_dataset.tokenization.modular.fst_load import MMT_FST_PATH\n",
    "\n",
    "print(f\"{MMT_FST_PATH = }\")\n",
    "print(f\"{MMT_FST_PATH.stat().st_size = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\n",
      "MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=X), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\n",
      "[ERROR]:  Tokenizer `MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=X), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))` not found in the list of tested tokenizers, and do_except = True. We found the following matches based on edit distance:\n",
      "edit dist 0 (should be empty?): []\n",
      "edit dist 1: ['MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))', 'MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=T), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))']\n"
     ]
    }
   ],
   "source": [
    "# we can also use `check_tokenizer_in_fst` manually, and if it cant find a tokenizer it will give us similar ones\n",
    "\n",
    "from maze_dataset.tokenization.modular.fst_load import check_tokenizer_in_fst\n",
    "\n",
    "print(mt_default.name)\n",
    "mt_name_modified: str = mt_default.name.replace(\n",
    "\t\"ConnectionEdges(walls=F),\", \"ConnectionEdges(walls=X),\"\n",
    ")\n",
    "print(mt_name_modified)\n",
    "try:\n",
    "\tcheck_tokenizer_in_fst(mt_name_modified, do_except=True)\n",
    "except Exception as e:  # noqa: BLE001\n",
    "\tprint(\"[ERROR]: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Tokenizer Collections\n",
    "\n",
    "There are a several practical ways to filter down a collection of tokenizers, or alternatively, generate a new collection with a filter.\n",
    "\n",
    "**WARNING: Applying `filter` to the output of `get_all_tokenizers` is extremely slow due to the size of the initial population. Only use the first 3 methods for filtering much smaller collections of tokenizers. To generate a new collection based on filters, always use `utils.all_instances`**\n",
    "\n",
    "In order of increasing speed, power and decreasing syntactic concision:\n",
    "\n",
    "1. `MazeTokenizerModular.has_element`\n",
    "    - Use case: Use with `filter` for concise, basic filtering on an existing collection\n",
    "1. `MazeTokenizerModular.tokenizer_elements`\n",
    "    - Use case: Use with `filter` for more precise filtering on an existing collection\n",
    "1. `MazeTokenizerModular.summary`\n",
    "    - Use case: Use with `filter` for more precise filtering on an existing collection\n",
    "1. `utils.all_instances`\n",
    "    - Use case: Generate a new collection with filter(s).\n",
    "    - Anytime you don't already have a small collection of tokenizers as the starting population.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all = len(get_all_tokenizers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 1: 13824 tokenizers / 5878656 tokenizers\n",
      "filtered 2: 27216 tokenizers / 5878656 tokenizers\n",
      "filtered 3: 979776 tokenizers / 5878656 tokenizers\n"
     ]
    }
   ],
   "source": [
    "filtered_1: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling `all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\tCoordTokenizers._CoordTokenizer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tCoordTokenizers.UT,\n",
    "\t\t\t),\n",
    "\t\t\tStepTokenizers.StepTokenizerPermutation: lambda x: x[0]\n",
    "\t\t\t== StepTokenizers.Cardinal()\n",
    "\t\t\tand len(x) < 3,\n",
    "\t\t\tAdjListTokenizers._AdjListTokenizer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tAdjListTokenizers.AdjListCardinal,\n",
    "\t\t\t),\n",
    "\t\t\tEdgeSubsets._EdgeSubset: lambda x: x\n",
    "\t\t\t== EdgeSubsets.ConnectionEdges(walls=False),\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "filtered_2: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling`all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\t_TokenizerElement: lambda x: x.is_valid()\n",
    "\t\t\tand not getattr(x, \"pre\", False)\n",
    "\t\t\tand not getattr(x, \"intra\", False)\n",
    "\t\t\tand not getattr(x, \"post\", False),  # Minimal delimiters everywhere...\n",
    "\t\t\tCoordTokenizers.CTT: lambda x: x.pre\n",
    "\t\t\tand x.intra\n",
    "\t\t\tand x.post,  # ...except for the coord tokens\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "filtered_3: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling `all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\tPromptSequencers._PromptSequencer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tPromptSequencers.AOTP,\n",
    "\t\t\t),\n",
    "\t\t\tTargetTokenizers._TargetTokenizer: lambda x: x\n",
    "\t\t\t== TargetTokenizers.Unlabeled(),\n",
    "\t\t\tStepSizes.Singles: lambda x: False,  # noqa: ARG005\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "print(f\"filtered 1: {len(filtered_1)} tokenizers / {len_all} tokenizers\")\n",
    "print(f\"filtered 2: {len(filtered_2)} tokenizers / {len_all} tokenizers\")\n",
    "print(f\"filtered 3: {len(filtered_3)} tokenizers / {len_all} tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples below show equivalent methods of filtering one of the smaller collections above using options 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered: 4608 tokenizers / 5878656 tokenizers\n",
      "set(filtered_has_element).symmetric_difference(set(filtered_summary)) = set()\n"
     ]
    }
   ],
   "source": [
    "filtered_has_element: list[MazeTokenizerModular] = list(\n",
    "\tfilter(lambda x: x.has_element(EdgePermuters.BothCoords()), filtered_1),\n",
    ")\n",
    "\n",
    "filtered_tokenizer_elements: list[MazeTokenizerModular] = list(\n",
    "\tfilter(lambda x: EdgePermuters.BothCoords() in x.tokenizer_elements, filtered_1),\n",
    ")\n",
    "\n",
    "filtered_summary: list[MazeTokenizerModular] = list(\n",
    "\tfilter(\n",
    "\t\tlambda x: x.summary()[\"edge_permuter\"] == EdgePermuters.BothCoords().name,\n",
    "\t\tfiltered_1,\n",
    "\t),\n",
    ")\n",
    "\n",
    "print(f\"filtered: {len(filtered_has_element)} tokenizers / {len_all} tokenizers\")\n",
    "\n",
    "assert set(filtered_has_element) == set(filtered_tokenizer_elements)\n",
    "print(f\"{set(filtered_has_element).symmetric_difference(set(filtered_summary)) = }\")\n",
    "assert set(filtered_has_element) == set(filtered_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenizerElement Behavior Reference\n",
    "\n",
    "For each primary `TokenizerElement`, tokenizations and encodings derived from the below maze are logged in DataFrames for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get the dataset 'test-g3-n1-a_dfs-h74622'\n",
      "generating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating & solving mazes: 100%|██████████| 1/1 [00:00<00:00, 756.28maze/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset test with 1 items. output.cfg.to_fname() = 'test-g3-n1-a_dfs-h74622'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg: MazeDatasetConfig = MazeDatasetConfig(\n",
    "\tname=\"test\",\n",
    "\tgrid_n=3,\n",
    "\tn_mazes=1,\n",
    "\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    ")\n",
    "dataset: MazeDataset = MazeDataset.from_config(\n",
    "\tcfg,\n",
    "\tdo_download=False,\n",
    "\tload_local=False,\n",
    "\tdo_generate=True,\n",
    "\tsave_local=False,\n",
    "\tverbose=True,\n",
    "\tgen_parallel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "mz: SolvedMaze = dataset[0]\n",
    "MazePlot(mz).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_elements_df(\n",
    "\telem_type: type[_TokenizerElement],\n",
    "\tencoding: bool = True,\n",
    "\t**to_tokens_kwargs,\n",
    ") -> pd.DataFrame:\n",
    "\tcolumns = [\"_TokenizerElement\", \"tokens\"]\n",
    "\tif encoding:\n",
    "\t\tcolumns.append(\"encoding\")\n",
    "\ttokenizers: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "\ttokenizers[\"_TokenizerElement\"] = list(\n",
    "\t\tall_instances(\n",
    "\t\t\telem_type,\n",
    "\t\t\tvalidation_funcs=MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,\n",
    "\t\t),\n",
    "\t)\n",
    "\ttokenizers[\"tokens\"] = tokenizers[\"_TokenizerElement\"].apply(\n",
    "\t\tlambda x: \" \".join(x.to_tokens(**to_tokens_kwargs)),\n",
    "\t)\n",
    "\tif encoding:\n",
    "\t\ttokenizers[\"encoding\"] = tokenizers[\"tokens\"].apply(\n",
    "\t\t\tlambda x: MazeTokenizerModular.encode(x),\n",
    "\t\t)\n",
    "\treturn tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CoordTokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_TokenizerElement</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UT()</td>\n",
       "      <td>(1,2)</td>\n",
       "      <td>[1602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTT(pre=T, intra=T, post=T)</td>\n",
       "      <td>( 1 , 2 )</td>\n",
       "      <td>[11, 321, 12, 322, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTT(pre=T, intra=T, post=F)</td>\n",
       "      <td>( 1 , 2</td>\n",
       "      <td>[11, 321, 12, 322]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTT(pre=T, intra=F, post=T)</td>\n",
       "      <td>( 1 2 )</td>\n",
       "      <td>[11, 321, 322, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTT(pre=T, intra=F, post=F)</td>\n",
       "      <td>( 1 2</td>\n",
       "      <td>[11, 321, 322]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CTT(pre=F, intra=T, post=T)</td>\n",
       "      <td>1 , 2 )</td>\n",
       "      <td>[321, 12, 322, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CTT(pre=F, intra=T, post=F)</td>\n",
       "      <td>1 , 2</td>\n",
       "      <td>[321, 12, 322]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CTT(pre=F, intra=F, post=T)</td>\n",
       "      <td>1 2 )</td>\n",
       "      <td>[321, 322, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CTT(pre=F, intra=F, post=F)</td>\n",
       "      <td>1 2</td>\n",
       "      <td>[321, 322]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             _TokenizerElement     tokens                encoding\n",
       "0                         UT()      (1,2)                  [1602]\n",
       "1  CTT(pre=T, intra=T, post=T)  ( 1 , 2 )  [11, 321, 12, 322, 13]\n",
       "2  CTT(pre=T, intra=T, post=F)    ( 1 , 2      [11, 321, 12, 322]\n",
       "3  CTT(pre=T, intra=F, post=T)    ( 1 2 )      [11, 321, 322, 13]\n",
       "4  CTT(pre=T, intra=F, post=F)      ( 1 2          [11, 321, 322]\n",
       "5  CTT(pre=F, intra=T, post=T)    1 , 2 )      [321, 12, 322, 13]\n",
       "6  CTT(pre=F, intra=T, post=F)      1 , 2          [321, 12, 322]\n",
       "7  CTT(pre=F, intra=F, post=T)      1 2 )          [321, 322, 13]\n",
       "8  CTT(pre=F, intra=F, post=F)        1 2              [321, 322]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_tokenizers = all_elements_df(\n",
    "\tCoordTokenizers._CoordTokenizer,\n",
    "\tcoord=mz.solution[0],\n",
    ")\n",
    "coord_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency List Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_TokenizerElement</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), SortedCoords())</td>\n",
       "      <td>&lt;XX&gt; (0,0) (0,1) ; &lt;--&gt; (1,1) (1,2) ; &lt;--&gt; (1,0) (2,0) ; &lt;--&gt; (1,2) (2,2) ; &lt;--&gt; (2,1) (2,2) ; &lt;--&gt; (0,1) (1,1) ; &lt;XX&gt; (1,0) (1,1) ; &lt;XX&gt; (0,1) (0,2) ; &lt;--&gt; (0,2) (1,2) ; &lt;--&gt; (2,0) (2,1) ; &lt;--&gt; (0,0) (1,0) ; &lt;XX&gt; (1,1) (2,1) ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), RandomCoords())</td>\n",
       "      <td>&lt;--&gt; (2,0) (1,0) ; &lt;XX&gt; (1,0) (1,1) ; &lt;--&gt; (1,0) (0,0) ; &lt;XX&gt; (1,1) (2,1) ; &lt;XX&gt; (0,2) (0,1) ; &lt;--&gt; (2,1) (2,0) ; &lt;--&gt; (1,2) (0,2) ; &lt;--&gt; (2,2) (1,2) ; &lt;--&gt; (0,1) (1,1) ; &lt;--&gt; (1,1) (1,2) ; &lt;--&gt; (2,1) (2,2) ; &lt;XX&gt; (0,0) (0,1) ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), BothCoords())</td>\n",
       "      <td>&lt;--&gt; (2,0) (2,1) ; &lt;--&gt; (1,0) (0,0) ; &lt;XX&gt; (0,1) (0,0) ; &lt;XX&gt; (2,1) (1,1) ; &lt;--&gt; (2,1) (2,0) ; &lt;--&gt; (1,2) (1,1) ; &lt;--&gt; (0,1) (1,1) ; &lt;--&gt; (2,0) (1,0) ; &lt;XX&gt; (0,2) (0,1) ; &lt;--&gt; (0,2) (1,2) ; &lt;--&gt; (0,0) (1,0) ; &lt;XX&gt; (1,1) (2,1) ; &lt;XX&gt; (0,0) (0,1) ; &lt;--&gt; (2,1) (2,2) ; &lt;XX&gt; (0,1) (0,2) ; &lt;--&gt; (2,2) (2,1) ; &lt;--&gt; (1,0) (2,0) ; &lt;XX&gt; (1,0) (1,1) ; &lt;--&gt; (1,1) (1,2) ; &lt;--&gt; (2,2) (1,2) ; &lt;XX&gt; (1,1) (1,0) ; &lt;--&gt; (1,2) (0,2) ; &lt;--&gt; (1,1) (0,1) ; &lt;--&gt; (1,2) (2,2) ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), ConnectionEdges(walls=T), SortedCoords())</td>\n",
       "      <td>&lt;XX&gt; (0,0) (0,1) ; &lt;XX&gt; (1,0) (1,1) ; &lt;XX&gt; (0,1) (0,2) ; &lt;XX&gt; (1,1) (2,1) ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), ConnectionEdges(walls=T), RandomCoords())</td>\n",
       "      <td>&lt;XX&gt; (0,2) (0,1) ; &lt;XX&gt; (2,1) (1,1) ; &lt;XX&gt; (1,0) (1,1) ; &lt;XX&gt; (0,1) (0,0) ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=T), RandomCoords())</td>\n",
       "      <td>(1,1) SOUTH &lt;XX&gt; (0,0) EAST &lt;XX&gt; (0,1) EAST &lt;XX&gt; (1,1) WEST &lt;XX&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=T), BothCoords())</td>\n",
       "      <td>(1,1) SOUTH &lt;XX&gt; (0,0) EAST &lt;XX&gt; (0,1) EAST &lt;XX&gt; (1,0) EAST &lt;XX&gt; (2,1) NORTH &lt;XX&gt; (0,1) WEST &lt;XX&gt; (0,2) WEST &lt;XX&gt; (1,1) WEST &lt;XX&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), SortedCoords())</td>\n",
       "      <td>(0,0) SOUTH &lt;--&gt; (0,1) SOUTH &lt;--&gt; (0,2) SOUTH &lt;--&gt; (1,0) SOUTH &lt;--&gt; (1,1) EAST &lt;--&gt; (1,2) SOUTH &lt;--&gt; (2,0) EAST &lt;--&gt; (2,1) EAST &lt;--&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), RandomCoords())</td>\n",
       "      <td>(1,0) NORTH &lt;--&gt; (1,1) NORTH &lt;--&gt; (0,2) SOUTH &lt;--&gt; (2,0) NORTH &lt;--&gt; (1,2) SOUTH &lt;--&gt; (1,1) EAST &lt;--&gt; (2,1) WEST &lt;--&gt; (2,2) WEST &lt;--&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), BothCoords())</td>\n",
       "      <td>(0,0) SOUTH &lt;--&gt; (0,1) SOUTH &lt;--&gt; (0,2) SOUTH &lt;--&gt; (1,0) SOUTH &lt;--&gt; (1,2) SOUTH &lt;--&gt; (1,1) EAST &lt;--&gt; (2,0) EAST &lt;--&gt; (2,1) EAST &lt;--&gt; (1,0) NORTH &lt;--&gt; (1,1) NORTH &lt;--&gt; (1,2) NORTH &lt;--&gt; (2,0) NORTH &lt;--&gt; (2,2) NORTH &lt;--&gt; (1,2) WEST &lt;--&gt; (2,1) WEST &lt;--&gt; (2,2) WEST &lt;--&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 _TokenizerElement  \\\n",
       "0              AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), SortedCoords())   \n",
       "1              AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), RandomCoords())   \n",
       "2                AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), AllLatticeEdges(), BothCoords())   \n",
       "3       AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), ConnectionEdges(walls=T), SortedCoords())   \n",
       "4       AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=0), ConnectionEdges(walls=T), RandomCoords())   \n",
       "..                                                                                                                             ...   \n",
       "211  AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=T), RandomCoords())   \n",
       "212    AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=T), BothCoords())   \n",
       "213  AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), SortedCoords())   \n",
       "214  AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), RandomCoords())   \n",
       "215    AdjListCardinal(pre=F, post=F, shuffle_d0=F, Ungrouped(connection_token_ordinal=2), ConnectionEdges(walls=F), BothCoords())   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                      tokens  \n",
       "0                                                                                                                                                                                                                                        <XX> (0,0) (0,1) ; <--> (1,1) (1,2) ; <--> (1,0) (2,0) ; <--> (1,2) (2,2) ; <--> (2,1) (2,2) ; <--> (0,1) (1,1) ; <XX> (1,0) (1,1) ; <XX> (0,1) (0,2) ; <--> (0,2) (1,2) ; <--> (2,0) (2,1) ; <--> (0,0) (1,0) ; <XX> (1,1) (2,1) ;  \n",
       "1                                                                                                                                                                                                                                        <--> (2,0) (1,0) ; <XX> (1,0) (1,1) ; <--> (1,0) (0,0) ; <XX> (1,1) (2,1) ; <XX> (0,2) (0,1) ; <--> (2,1) (2,0) ; <--> (1,2) (0,2) ; <--> (2,2) (1,2) ; <--> (0,1) (1,1) ; <--> (1,1) (1,2) ; <--> (2,1) (2,2) ; <XX> (0,0) (0,1) ;  \n",
       "2    <--> (2,0) (2,1) ; <--> (1,0) (0,0) ; <XX> (0,1) (0,0) ; <XX> (2,1) (1,1) ; <--> (2,1) (2,0) ; <--> (1,2) (1,1) ; <--> (0,1) (1,1) ; <--> (2,0) (1,0) ; <XX> (0,2) (0,1) ; <--> (0,2) (1,2) ; <--> (0,0) (1,0) ; <XX> (1,1) (2,1) ; <XX> (0,0) (0,1) ; <--> (2,1) (2,2) ; <XX> (0,1) (0,2) ; <--> (2,2) (2,1) ; <--> (1,0) (2,0) ; <XX> (1,0) (1,1) ; <--> (1,1) (1,2) ; <--> (2,2) (1,2) ; <XX> (1,1) (1,0) ; <--> (1,2) (0,2) ; <--> (1,1) (0,1) ; <--> (1,2) (2,2) ;  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                <XX> (0,0) (0,1) ; <XX> (1,0) (1,1) ; <XX> (0,1) (0,2) ; <XX> (1,1) (2,1) ;  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                <XX> (0,2) (0,1) ; <XX> (2,1) (1,1) ; <XX> (1,0) (1,1) ; <XX> (0,1) (0,0) ;  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...  \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                         (1,1) SOUTH <XX> (0,0) EAST <XX> (0,1) EAST <XX> (1,1) WEST <XX>  \n",
       "212                                                                                                                                                                                                                                                                                                                                        (1,1) SOUTH <XX> (0,0) EAST <XX> (0,1) EAST <XX> (1,0) EAST <XX> (2,1) NORTH <XX> (0,1) WEST <XX> (0,2) WEST <XX> (1,1) WEST <XX>  \n",
       "213                                                                                                                                                                                                                                                                                                                                     (0,0) SOUTH <--> (0,1) SOUTH <--> (0,2) SOUTH <--> (1,0) SOUTH <--> (1,1) EAST <--> (1,2) SOUTH <--> (2,0) EAST <--> (2,1) EAST <-->  \n",
       "214                                                                                                                                                                                                                                                                                                                                     (1,0) NORTH <--> (1,1) NORTH <--> (0,2) SOUTH <--> (2,0) NORTH <--> (1,2) SOUTH <--> (1,1) EAST <--> (2,1) WEST <--> (2,2) WEST <-->  \n",
       "215                                                                                                                                                                                                (0,0) SOUTH <--> (0,1) SOUTH <--> (0,2) SOUTH <--> (1,0) SOUTH <--> (1,2) SOUTH <--> (1,1) EAST <--> (2,0) EAST <--> (2,1) EAST <--> (1,0) NORTH <--> (1,1) NORTH <--> (1,2) NORTH <--> (2,0) NORTH <--> (2,2) NORTH <--> (1,2) WEST <--> (2,1) WEST <--> (2,2) WEST <-->  \n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjlist_tokenizers = all_elements_df(\n",
    "\tAdjListTokenizers._AdjListTokenizer,\n",
    "\tencoding=False,\n",
    "\tmaze=mz,\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "adjlist_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_TokenizerElement</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unlabeled(post=T)</td>\n",
       "      <td>(0,0) ||</td>\n",
       "      <td>[1596, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlabeled(post=F)</td>\n",
       "      <td>(0,0)</td>\n",
       "      <td>[1596]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _TokenizerElement    tokens    encoding\n",
       "0  Unlabeled(post=T)  (0,0) ||  [1596, 15]\n",
       "1  Unlabeled(post=F)     (0,0)      [1596]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokenizers = all_elements_df(\n",
    "\tTargetTokenizers._TargetTokenizer,\n",
    "\ttargets=[mz.end_pos],\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "target_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_TokenizerElement</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=T, post=T)</td>\n",
       "      <td>STEP (1,2) : STEP (2,2) : THEN STEP (2,1) : THEN STEP (2,0) : THEN STEP (1,0) : THEN STEP (0,0) : THEN</td>\n",
       "      <td>[704, 1602, 16, 704, 1604, 16, 17, 704, 1603, 16, 17, 704, 1601, 16, 17, 704, 1598, 16, 17, 704, 1596, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=T, post=F)</td>\n",
       "      <td>STEP (1,2) : STEP (2,2) : STEP (2,1) : STEP (2,0) : STEP (1,0) : STEP (0,0) :</td>\n",
       "      <td>[704, 1602, 16, 704, 1604, 16, 704, 1603, 16, 704, 1601, 16, 704, 1598, 16, 704, 1596, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=F, post=T)</td>\n",
       "      <td>STEP (1,2) STEP (2,2) THEN STEP (2,1) THEN STEP (2,0) THEN STEP (1,0) THEN STEP (0,0) THEN</td>\n",
       "      <td>[704, 1602, 704, 1604, 17, 704, 1603, 17, 704, 1601, 17, 704, 1598, 17, 704, 1596, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=F, post=F)</td>\n",
       "      <td>STEP (1,2) STEP (2,2) STEP (2,1) STEP (2,0) STEP (1,0) STEP (0,0)</td>\n",
       "      <td>[704, 1602, 704, 1604, 704, 1603, 704, 1601, 704, 1598, 704, 1596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=T, post=T)</td>\n",
       "      <td>(1,2) : (2,2) : THEN (2,1) : THEN (2,0) : THEN (1,0) : THEN (0,0) : THEN</td>\n",
       "      <td>[1602, 16, 1604, 16, 17, 1603, 16, 17, 1601, 16, 17, 1598, 16, 17, 1596, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=T, intra=F, post=F)</td>\n",
       "      <td>STEP (1,2) STEP +5 BACKWARD SOUTH (0,0)</td>\n",
       "      <td>[704, 1602, 704, 69, 60, 56, 1596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=T, post=T)</td>\n",
       "      <td>(1,2) : +5 : BACKWARD : SOUTH : (0,0) : THEN</td>\n",
       "      <td>[1602, 16, 69, 16, 60, 16, 56, 16, 1596, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=T, post=F)</td>\n",
       "      <td>(1,2) : +5 : BACKWARD : SOUTH : (0,0) :</td>\n",
       "      <td>[1602, 16, 69, 16, 60, 16, 56, 16, 1596, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=F, post=T)</td>\n",
       "      <td>(1,2) +5 BACKWARD SOUTH (0,0) THEN</td>\n",
       "      <td>[1602, 69, 60, 56, 1596, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=F, post=F)</td>\n",
       "      <td>(1,2) +5 BACKWARD SOUTH (0,0)</td>\n",
       "      <td>[1602, 69, 60, 56, 1596]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1008 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   _TokenizerElement  \\\n",
       "0                                       StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=T, post=T)   \n",
       "1                                       StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=T, post=F)   \n",
       "2                                       StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=F, post=T)   \n",
       "3                                       StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=T, intra=F, post=F)   \n",
       "4                                       StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=T, post=T)   \n",
       "...                                                                                                              ...   \n",
       "1003  StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=T, intra=F, post=F)   \n",
       "1004  StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=T, post=T)   \n",
       "1005  StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=T, post=F)   \n",
       "1006  StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=F, post=T)   \n",
       "1007  StepSequence(Forks(), step_tokenizers=(Distance(), Relative(), Cardinal(), Coord(), ), pre=F, intra=F, post=F)   \n",
       "\n",
       "                                                                                                      tokens  \\\n",
       "0     STEP (1,2) : STEP (2,2) : THEN STEP (2,1) : THEN STEP (2,0) : THEN STEP (1,0) : THEN STEP (0,0) : THEN   \n",
       "1                              STEP (1,2) : STEP (2,2) : STEP (2,1) : STEP (2,0) : STEP (1,0) : STEP (0,0) :   \n",
       "2                 STEP (1,2) STEP (2,2) THEN STEP (2,1) THEN STEP (2,0) THEN STEP (1,0) THEN STEP (0,0) THEN   \n",
       "3                                          STEP (1,2) STEP (2,2) STEP (2,1) STEP (2,0) STEP (1,0) STEP (0,0)   \n",
       "4                                   (1,2) : (2,2) : THEN (2,1) : THEN (2,0) : THEN (1,0) : THEN (0,0) : THEN   \n",
       "...                                                                                                      ...   \n",
       "1003                                                                 STEP (1,2) STEP +5 BACKWARD SOUTH (0,0)   \n",
       "1004                                                            (1,2) : +5 : BACKWARD : SOUTH : (0,0) : THEN   \n",
       "1005                                                                 (1,2) : +5 : BACKWARD : SOUTH : (0,0) :   \n",
       "1006                                                                      (1,2) +5 BACKWARD SOUTH (0,0) THEN   \n",
       "1007                                                                           (1,2) +5 BACKWARD SOUTH (0,0)   \n",
       "\n",
       "                                                                                                            encoding  \n",
       "0     [704, 1602, 16, 704, 1604, 16, 17, 704, 1603, 16, 17, 704, 1601, 16, 17, 704, 1598, 16, 17, 704, 1596, 16, 17]  \n",
       "1                         [704, 1602, 16, 704, 1604, 16, 704, 1603, 16, 704, 1601, 16, 704, 1598, 16, 704, 1596, 16]  \n",
       "2                             [704, 1602, 704, 1604, 17, 704, 1603, 17, 704, 1601, 17, 704, 1598, 17, 704, 1596, 17]  \n",
       "3                                                 [704, 1602, 704, 1604, 704, 1603, 704, 1601, 704, 1598, 704, 1596]  \n",
       "4                                   [1602, 16, 1604, 16, 17, 1603, 16, 17, 1601, 16, 17, 1598, 16, 17, 1596, 16, 17]  \n",
       "...                                                                                                              ...  \n",
       "1003                                                                              [704, 1602, 704, 69, 60, 56, 1596]  \n",
       "1004                                                                [1602, 16, 69, 16, 60, 16, 56, 16, 1596, 16, 17]  \n",
       "1005                                                                    [1602, 16, 69, 16, 60, 16, 56, 16, 1596, 16]  \n",
       "1006                                                                                    [1602, 69, 60, 56, 1596, 17]  \n",
       "1007                                                                                        [1602, 69, 60, 56, 1596]  \n",
       "\n",
       "[1008 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tokenizers = all_elements_df(\n",
    "\tPathTokenizers._PathTokenizer,\n",
    "\tmaze=mz,\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "path_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Sequencers\n",
    "\n",
    "Currently, the only difference in possible prompt sequencers is the inclusion/exclusion of target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_TokenizerElement</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))</td>\n",
       "      <td>&lt;ADJLIST_START&gt; (0,1) &lt;--&gt; (1,1) ; (2,1) &lt;--&gt; (2,0) ; (2,2) &lt;--&gt; (1,2) ; (2,2) &lt;--&gt; (2,1) ; (0,2) &lt;--&gt; (1,2) ; (0,0) &lt;--&gt; (1,0) ; (1,1) &lt;--&gt; (1,2) ; (1,0) &lt;--&gt; (2,0) ; &lt;ADJLIST_END&gt; &lt;ORIGIN_START&gt; (1,2) &lt;ORIGIN_END&gt; &lt;TARGET_START&gt; (0,0) &lt;TARGET_END&gt; &lt;PATH_START&gt; (1,2) (2,2) (2,1) (2,0) (1,0) (0,0) &lt;PATH_END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))</td>\n",
       "      <td>&lt;ADJLIST_START&gt; (1,0) &lt;--&gt; (2,0) ; (0,1) &lt;--&gt; (1,1) ; (1,0) &lt;--&gt; (0,0) ; (2,1) &lt;--&gt; (2,2) ; (1,2) &lt;--&gt; (2,2) ; (2,1) &lt;--&gt; (2,0) ; (1,2) &lt;--&gt; (0,2) ; (1,1) &lt;--&gt; (1,2) ; &lt;ADJLIST_END&gt; &lt;ORIGIN_START&gt; (1,2) &lt;ORIGIN_END&gt; &lt;TARGET_START&gt; &lt;TARGET_END&gt; &lt;PATH_START&gt; (1,2) (2,2) (2,1) (2,0) (1,0) (0,0) &lt;PATH_END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                         _TokenizerElement  \\\n",
       "0  AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))   \n",
       "1                      AOP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                  tokens  \n",
       "0  <ADJLIST_START> (0,1) <--> (1,1) ; (2,1) <--> (2,0) ; (2,2) <--> (1,2) ; (2,2) <--> (2,1) ; (0,2) <--> (1,2) ; (0,0) <--> (1,0) ; (1,1) <--> (1,2) ; (1,0) <--> (2,0) ; <ADJLIST_END> <ORIGIN_START> (1,2) <ORIGIN_END> <TARGET_START> (0,0) <TARGET_END> <PATH_START> (1,2) (2,2) (2,1) (2,0) (1,0) (0,0) <PATH_END>  \n",
       "1        <ADJLIST_START> (1,0) <--> (2,0) ; (0,1) <--> (1,1) ; (1,0) <--> (0,0) ; (2,1) <--> (2,2) ; (1,2) <--> (2,2) ; (2,1) <--> (2,0) ; (1,2) <--> (0,2) ; (1,1) <--> (1,2) ; <ADJLIST_END> <ORIGIN_START> (1,2) <ORIGIN_END> <TARGET_START> <TARGET_END> <PATH_START> (1,2) (2,2) (2,1) (2,0) (1,0) (0,0) <PATH_END>  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_sequencers = [PromptSequencers.AOTP(), PromptSequencers.AOP()]\n",
    "columns = [\"_TokenizerElement\", \"tokens\"]\n",
    "tokenizers: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "tokenizers[\"_TokenizerElement\"] = prompt_sequencers\n",
    "tokenizers[\"tokens\"] = tokenizers[\"_TokenizerElement\"].apply(\n",
    "\tlambda x: \" \".join(x.to_tokens(maze=mz)),\n",
    ")\n",
    "tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample of `MazeTokenizerModular`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_size: int = 1_000\n",
    "\n",
    "tokenizers: list[MazeTokenizerModular] = random.sample(\n",
    "\tget_all_tokenizers(),\n",
    "\trandom_sample_size,\n",
    ")\n",
    "columns = [\"MazeTokenizerModular\", \"tokens\", \"encoding\", *mt_default.summary().keys()]\n",
    "df: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "df[\"MazeTokenizerModular\"] = tokenizers\n",
    "df[\"tokens\"] = df[\"MazeTokenizerModular\"].apply(\n",
    "\tlambda x: \" \".join(x.to_tokens(maze=mz)),\n",
    ")\n",
    "df.encoding = df.tokens.apply(MazeTokenizerModular.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizers: 100%|██████████| 10/10 [00:00<00:00, 25.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MazeTokenizerModular</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoding</th>\n",
       "      <th>prompt_sequencer</th>\n",
       "      <th>coord_tokenizer</th>\n",
       "      <th>adj_list_tokenizer</th>\n",
       "      <th>edge_grouping</th>\n",
       "      <th>edge_subset</th>\n",
       "      <th>edge_permuter</th>\n",
       "      <th>target_tokenizer</th>\n",
       "      <th>path_tokenizer</th>\n",
       "      <th>step_size</th>\n",
       "      <th>step_tokenizers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; 1 2 &lt;--&gt; 2 2 ; 2 0 &lt;--&gt; 2 1 ; ...</td>\n",
       "      <td>[0, 321, 322, 8, 322, 322, 9, 322, 320, 8, 322...</td>\n",
       "      <td>AOP(CTT(pre=F, intra=F, post=F), AdjListCoord(...</td>\n",
       "      <td>CTT(pre=F, intra=F, post=F)</td>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=1)</td>\n",
       "      <td>ConnectionEdges(walls=F)</td>\n",
       "      <td>SortedCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Cardina...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Cardinal()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;XX&gt; ( 1 1 ) ( 1 0 ) ; &lt;--&gt; ( ...</td>\n",
       "      <td>[0, 707, 11, 321, 321, 13, 11, 321, 320, 13, 9...</td>\n",
       "      <td>AOP(CTT(pre=T, intra=F, post=T), AdjListCoord(...</td>\n",
       "      <td>CTT(pre=T, intra=F, post=T)</td>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>BothCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Cardina...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Relative()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; ( 0 , 0 ) &lt;XX&gt; EAST ; ( 0 , 2 ...</td>\n",
       "      <td>[0, 11, 320, 12, 320, 13, 707, 57, 9, 11, 320,...</td>\n",
       "      <td>AOTP(CTT(pre=T, intra=T, post=T), AdjListCardi...</td>\n",
       "      <td>CTT(pre=T, intra=T, post=T)</td>\n",
       "      <td>AdjListCardinal(pre=F, post=T, shuffle_d0=F, U...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=1)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>RandomCoords()</td>\n",
       "      <td>Unlabeled(post=F)</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Coord()...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Coord()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;--&gt; ( 1 1 EAST &lt;XX&gt; ( 0 0 EAS...</td>\n",
       "      <td>[0, 8, 11, 321, 321, 57, 707, 11, 320, 320, 57...</td>\n",
       "      <td>AOP(CTT(pre=T, intra=F, post=F), AdjListCardin...</td>\n",
       "      <td>CTT(pre=T, intra=F, post=F)</td>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>SortedCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Dista...</td>\n",
       "      <td>Singles()</td>\n",
       "      <td>Coord()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;XX&gt; 0 0 0 1 ; &lt;--&gt; 0 0 1 0 ; ...</td>\n",
       "      <td>[0, 707, 320, 320, 320, 321, 9, 8, 320, 320, 3...</td>\n",
       "      <td>AOTP(CTT(pre=F, intra=F, post=F), AdjListCoord...</td>\n",
       "      <td>CTT(pre=F, intra=F, post=F)</td>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=F, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>SortedCoords()</td>\n",
       "      <td>Unlabeled(post=F)</td>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Relat...</td>\n",
       "      <td>Singles()</td>\n",
       "      <td>Coord()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;--&gt; 1 0 ) SOUTH &lt;XX&gt; 1 1 ) SO...</td>\n",
       "      <td>[0, 8, 321, 320, 13, 56, 707, 321, 321, 13, 56...</td>\n",
       "      <td>AOTP(CTT(pre=F, intra=F, post=T), AdjListCardi...</td>\n",
       "      <td>CTT(pre=F, intra=F, post=T)</td>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>SortedCoords()</td>\n",
       "      <td>Unlabeled(post=F)</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Cardina...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Coord()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; 1 , 0 ) 0 , 0 ) &lt;--&gt; ; 0 , 1 )...</td>\n",
       "      <td>[0, 321, 12, 320, 13, 320, 12, 320, 13, 8, 9, ...</td>\n",
       "      <td>AOTP(CTT(pre=F, intra=T, post=T), AdjListCoord...</td>\n",
       "      <td>CTT(pre=F, intra=T, post=T)</td>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=F, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=2)</td>\n",
       "      <td>ConnectionEdges(walls=F)</td>\n",
       "      <td>RandomCoords()</td>\n",
       "      <td>Unlabeled(post=F)</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Coord()...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Cardinal()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;--&gt; ( 2 , 0 ) ( 2 , 1 ) &lt;--&gt; ...</td>\n",
       "      <td>[0, 8, 11, 322, 12, 320, 13, 11, 322, 12, 321,...</td>\n",
       "      <td>AOP(CTT(pre=T, intra=T, post=T), AdjListCoord(...</td>\n",
       "      <td>CTT(pre=T, intra=T, post=T)</td>\n",
       "      <td>AdjListCoord(pre=F, post=F, shuffle_d0=T, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>ConnectionEdges(walls=F)</td>\n",
       "      <td>SortedCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Relativ...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Coord()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; ( 2 , 0 &lt;--&gt; NORTH ( 2 , 2 &lt;--...</td>\n",
       "      <td>[0, 11, 322, 12, 320, 8, 55, 11, 322, 12, 322,...</td>\n",
       "      <td>AOP(CTT(pre=T, intra=T, post=F), AdjListCardin...</td>\n",
       "      <td>CTT(pre=T, intra=T, post=F)</td>\n",
       "      <td>AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=1)</td>\n",
       "      <td>AllLatticeEdges()</td>\n",
       "      <td>BothCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Forks(), step_tokenizers=(Relativ...</td>\n",
       "      <td>Forks()</td>\n",
       "      <td>Distance()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>MazeTokenizerModular(prompt_sequencer=PromptSe...</td>\n",
       "      <td>&lt;ADJLIST_START&gt; &lt;XX&gt; ( 0 , 2 ) ( 0 , 1 ) ; &lt;XX...</td>\n",
       "      <td>[0, 707, 11, 320, 12, 322, 13, 11, 320, 12, 32...</td>\n",
       "      <td>AOP(CTT(pre=T, intra=T, post=T), AdjListCoord(...</td>\n",
       "      <td>CTT(pre=T, intra=T, post=T)</td>\n",
       "      <td>AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...</td>\n",
       "      <td>Ungrouped(connection_token_ordinal=0)</td>\n",
       "      <td>ConnectionEdges(walls=T)</td>\n",
       "      <td>BothCoords()</td>\n",
       "      <td>None</td>\n",
       "      <td>StepSequence(Singles(), step_tokenizers=(Cardi...</td>\n",
       "      <td>Singles()</td>\n",
       "      <td>Distance()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  MazeTokenizerModular  \\\n",
       "0    MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "1    MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "2    MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "3    MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "4    MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "..                                                 ...   \n",
       "995  MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "996  MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "997  MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "998  MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "999  MazeTokenizerModular(prompt_sequencer=PromptSe...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    <ADJLIST_START> 1 2 <--> 2 2 ; 2 0 <--> 2 1 ; ...   \n",
       "1    <ADJLIST_START> <XX> ( 1 1 ) ( 1 0 ) ; <--> ( ...   \n",
       "2    <ADJLIST_START> ( 0 , 0 ) <XX> EAST ; ( 0 , 2 ...   \n",
       "3    <ADJLIST_START> <--> ( 1 1 EAST <XX> ( 0 0 EAS...   \n",
       "4    <ADJLIST_START> <XX> 0 0 0 1 ; <--> 0 0 1 0 ; ...   \n",
       "..                                                 ...   \n",
       "995  <ADJLIST_START> <--> 1 0 ) SOUTH <XX> 1 1 ) SO...   \n",
       "996  <ADJLIST_START> 1 , 0 ) 0 , 0 ) <--> ; 0 , 1 )...   \n",
       "997  <ADJLIST_START> <--> ( 2 , 0 ) ( 2 , 1 ) <--> ...   \n",
       "998  <ADJLIST_START> ( 2 , 0 <--> NORTH ( 2 , 2 <--...   \n",
       "999  <ADJLIST_START> <XX> ( 0 , 2 ) ( 0 , 1 ) ; <XX...   \n",
       "\n",
       "                                              encoding  \\\n",
       "0    [0, 321, 322, 8, 322, 322, 9, 322, 320, 8, 322...   \n",
       "1    [0, 707, 11, 321, 321, 13, 11, 321, 320, 13, 9...   \n",
       "2    [0, 11, 320, 12, 320, 13, 707, 57, 9, 11, 320,...   \n",
       "3    [0, 8, 11, 321, 321, 57, 707, 11, 320, 320, 57...   \n",
       "4    [0, 707, 320, 320, 320, 321, 9, 8, 320, 320, 3...   \n",
       "..                                                 ...   \n",
       "995  [0, 8, 321, 320, 13, 56, 707, 321, 321, 13, 56...   \n",
       "996  [0, 321, 12, 320, 13, 320, 12, 320, 13, 8, 9, ...   \n",
       "997  [0, 8, 11, 322, 12, 320, 13, 11, 322, 12, 321,...   \n",
       "998  [0, 11, 322, 12, 320, 8, 55, 11, 322, 12, 322,...   \n",
       "999  [0, 707, 11, 320, 12, 322, 13, 11, 320, 12, 32...   \n",
       "\n",
       "                                      prompt_sequencer  \\\n",
       "0    AOP(CTT(pre=F, intra=F, post=F), AdjListCoord(...   \n",
       "1    AOP(CTT(pre=T, intra=F, post=T), AdjListCoord(...   \n",
       "2    AOTP(CTT(pre=T, intra=T, post=T), AdjListCardi...   \n",
       "3    AOP(CTT(pre=T, intra=F, post=F), AdjListCardin...   \n",
       "4    AOTP(CTT(pre=F, intra=F, post=F), AdjListCoord...   \n",
       "..                                                 ...   \n",
       "995  AOTP(CTT(pre=F, intra=F, post=T), AdjListCardi...   \n",
       "996  AOTP(CTT(pre=F, intra=T, post=T), AdjListCoord...   \n",
       "997  AOP(CTT(pre=T, intra=T, post=T), AdjListCoord(...   \n",
       "998  AOP(CTT(pre=T, intra=T, post=F), AdjListCardin...   \n",
       "999  AOP(CTT(pre=T, intra=T, post=T), AdjListCoord(...   \n",
       "\n",
       "                 coord_tokenizer  \\\n",
       "0    CTT(pre=F, intra=F, post=F)   \n",
       "1    CTT(pre=T, intra=F, post=T)   \n",
       "2    CTT(pre=T, intra=T, post=T)   \n",
       "3    CTT(pre=T, intra=F, post=F)   \n",
       "4    CTT(pre=F, intra=F, post=F)   \n",
       "..                           ...   \n",
       "995  CTT(pre=F, intra=F, post=T)   \n",
       "996  CTT(pre=F, intra=T, post=T)   \n",
       "997  CTT(pre=T, intra=T, post=T)   \n",
       "998  CTT(pre=T, intra=T, post=F)   \n",
       "999  CTT(pre=T, intra=T, post=T)   \n",
       "\n",
       "                                    adj_list_tokenizer  \\\n",
       "0    AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...   \n",
       "1    AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...   \n",
       "2    AdjListCardinal(pre=F, post=T, shuffle_d0=F, U...   \n",
       "3    AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...   \n",
       "4    AdjListCoord(pre=F, post=T, shuffle_d0=F, Ungr...   \n",
       "..                                                 ...   \n",
       "995  AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...   \n",
       "996  AdjListCoord(pre=F, post=T, shuffle_d0=F, Ungr...   \n",
       "997  AdjListCoord(pre=F, post=F, shuffle_d0=T, Ungr...   \n",
       "998  AdjListCardinal(pre=F, post=F, shuffle_d0=T, U...   \n",
       "999  AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungr...   \n",
       "\n",
       "                             edge_grouping               edge_subset  \\\n",
       "0    Ungrouped(connection_token_ordinal=1)  ConnectionEdges(walls=F)   \n",
       "1    Ungrouped(connection_token_ordinal=0)         AllLatticeEdges()   \n",
       "2    Ungrouped(connection_token_ordinal=1)         AllLatticeEdges()   \n",
       "3    Ungrouped(connection_token_ordinal=0)         AllLatticeEdges()   \n",
       "4    Ungrouped(connection_token_ordinal=0)         AllLatticeEdges()   \n",
       "..                                     ...                       ...   \n",
       "995  Ungrouped(connection_token_ordinal=0)         AllLatticeEdges()   \n",
       "996  Ungrouped(connection_token_ordinal=2)  ConnectionEdges(walls=F)   \n",
       "997  Ungrouped(connection_token_ordinal=0)  ConnectionEdges(walls=F)   \n",
       "998  Ungrouped(connection_token_ordinal=1)         AllLatticeEdges()   \n",
       "999  Ungrouped(connection_token_ordinal=0)  ConnectionEdges(walls=T)   \n",
       "\n",
       "      edge_permuter   target_tokenizer  \\\n",
       "0    SortedCoords()               None   \n",
       "1      BothCoords()               None   \n",
       "2    RandomCoords()  Unlabeled(post=F)   \n",
       "3    SortedCoords()               None   \n",
       "4    SortedCoords()  Unlabeled(post=F)   \n",
       "..              ...                ...   \n",
       "995  SortedCoords()  Unlabeled(post=F)   \n",
       "996  RandomCoords()  Unlabeled(post=F)   \n",
       "997  SortedCoords()               None   \n",
       "998    BothCoords()               None   \n",
       "999    BothCoords()               None   \n",
       "\n",
       "                                        path_tokenizer  step_size  \\\n",
       "0    StepSequence(Forks(), step_tokenizers=(Cardina...    Forks()   \n",
       "1    StepSequence(Forks(), step_tokenizers=(Cardina...    Forks()   \n",
       "2    StepSequence(Forks(), step_tokenizers=(Coord()...    Forks()   \n",
       "3    StepSequence(Singles(), step_tokenizers=(Dista...  Singles()   \n",
       "4    StepSequence(Singles(), step_tokenizers=(Relat...  Singles()   \n",
       "..                                                 ...        ...   \n",
       "995  StepSequence(Forks(), step_tokenizers=(Cardina...    Forks()   \n",
       "996  StepSequence(Forks(), step_tokenizers=(Coord()...    Forks()   \n",
       "997  StepSequence(Forks(), step_tokenizers=(Relativ...    Forks()   \n",
       "998  StepSequence(Forks(), step_tokenizers=(Relativ...    Forks()   \n",
       "999  StepSequence(Singles(), step_tokenizers=(Cardi...  Singles()   \n",
       "\n",
       "    step_tokenizers  \n",
       "0        Cardinal()  \n",
       "1        Relative()  \n",
       "2           Coord()  \n",
       "3           Coord()  \n",
       "4           Coord()  \n",
       "..              ...  \n",
       "995         Coord()  \n",
       "996      Cardinal()  \n",
       "997         Coord()  \n",
       "998      Distance()  \n",
       "999      Distance()  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in tqdm(\n",
    "\tmt_default.summary().keys(),\n",
    "\tdesc=\"Tokenizers\",\n",
    "\ttotal=len(mt_default.summary()),\n",
    "):\n",
    "\tdf[k] = df.apply(\n",
    "\t\tlambda x: x.MazeTokenizerModular.summary().get(k, None),  # noqa: B023\n",
    "\t\taxis=1,\n",
    "\t)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
